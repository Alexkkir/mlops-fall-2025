Задание 1

 
Выберите тему ML-проекта и сформулируйте бизнес-цель.


Определите целевые метрики для продакшена, например:
Среднее время отклика сервиса ≤ 200 мс
Доля неуспешных запросов ≤ 1 %
Использование памяти/CPU — в пределах SLA
Качество модели: точность ≥ 90 % или RMSE ≤ 0.1
Создайте репозиторий (предпочтительно GitHub).
В README опишите:
Цель проекта
Набор данных
План экспериментов
 

Итого, на этой неделе у вас должно сформироваться понимание того, какой проект вы реализуете, на чем обучаетесь, к чему стремитесь.

 

 

Задание 2


Реализуйте код для подготовки данных и обучения модели. Код должен быть организован как прототип, близкий к тому, что используется в реальных проектах после этапа экспериментов.

 

В проекте должны быть реализованы следующие этапы:

Загрузка и предобработка данных
Определение архитектуры модели на основе нейронных сетей
Запуск обучения
Сохранение обученной модели
Проверка и базовая валидация обученной модели (хотя бы выводом метрик в консольку)
 

Обязательно реализуйте конфигурационный файл, из которого читаются параметры обучения (путь к данным, random_seed, LR и прочее)
При обучении модели с нуля — нужен конфигурационный файл с архитектурой
Реализуйте .py файл для запуска обучения, который берет в качестве аргумента конфигурационный файл и возможные дополнительные значения (e.g. verbose)
Реализуйте логирование через пакет logging
Итоговая модель должна быть сохранена в формате, совместимом с Hugging Face, например с помощью метода save_pretrained().
 

Качество решения может быть базовым — важно, чтобы:

модель корректно обучалась и выдавала осмысленные предсказания;
процесс можно было полностью воспроизвести из кода репозитория.
 

Итого, у вас должна быть написана кодовая база для предобработки данных, обучения модели и простая валидация модели. 

Задание 3

 

Перепишите код обучения так, чтобы его можно было покрыть тестами. Код должен быть разбит на небольшие логические блоки так, чтобы было легко проверить корректность каждого отдельного блока

 

Тесты должны проверять корректность предобработки и работы пайплайна обучения, а не качество модели
Добавьте проверки для данных, включая:
соответствие формата и структуры;
правильность типов и диапазонов значений;
наличие необходимых признаков и целевых меток.
Добавьте проверку перевода предсказаний в корректные значения для потенциального API:
корректную обработку сырых результатов от модели;
корректный перевод в требуемые значения (вероятности классов → итоговый класс).
Настройте автоматический запуск всех тестов при каждом коммите (например, через GitHub Actions или аналогичный CI-инструмент).
Убедитесь, что все тесты проходят успешно и результаты логируются в системе CI.
Опционально: добавьте дополнительные автопроверки (линтеры, форматирование кода и т.п.).
 

Итого, на этой неделе ваш репозиторий должен начать самостоятельно проводить тесты при каждом новом коммите.

Критерии оценки (максимум - 10 базовых баллов и 3 бонусных)

 

Формулировка проекта и метрик — до 1 балла
Чёткая и обоснованная бизнес-цель вашего ML-проекта
Понятные целевые метрики (включая бизнесовые и технические)
Логичная связь между бизнес-задачей и выбранным ML-инструментом
Качество кода и архитектуры обучения — до 3 баллов
Оптимальность кода и алгоритмов (к примеру, использование векторных операций на этапе предобработки, если возможно)
Код структурирован, модули и функции разделены по логике
Понятные названия переменных и функций
Использование конфигов / параметров
Хорошее логирование процесса обучения
Наличие файла requirements или аналога, фиксирующего версии библиотек
Описание процесса запуска в README
Воспроизводимость процесса обучения (фиксация random_seed)
Валидация и предобработка данных — до 2 баллов
Проверка корректности входных данных (типы, пропуски, форматы)
Наличие базовой статистики для оценки корректности данных
Тестирование — до 2 баллов
Тесты покрывают ключевые функции пайплайна
Проверяются граничные случаи и ошибки
Автоматизация — до 2 баллов
Настроен автоматический запуск тестов при коммитах
Показано что при некорректном коммите тесты выдают ошибку
Возможные бонусы — до +3 баллов

Если сделаете что-то дополнительное, что нас впечатлит :)
 

 

Критические проблемы, за которые будут существенно снижены баллы за всё задание:

 

Не осуществлена достаточная воспроизводимость процесса обучения модели и получения данных
По описанию из README не удается запустить ваш код
 

Сдача задания

 

Для сдачи задания в anytask необходимо прислать ссылку на ВЕТКУ вашего репозитория, в котором выполнены указанные задания. Временем сдачи считается время, когда вы совершили последний push в эту ветку (именно push, а не локальный коммит). Основная ветка репозитория (master/main) проверяться НЕ БУДЕТ.
 

Штраф за дедлайн

 

За каждый час просрока будет сниматься 0.02 балла за задание, но не более 50% за всё задание
 

Задание 4


Подключаем DVC и версионируем данные/модели

Инициализируйте DVC в корне проекта - dvc init
Добавьте в контроль версий
сырой датасет
итоговую модель
файлы/директории, которые должны игнорироваться гитом, пропишите в .gitignore (DVC сделает это автоматически).
Создайте dvc.yaml/dvc.lock с пайплайном:
Stageы: prepare:    / train:    / evaluate:     –    предобработка данны/обучение/валидация
Подключите удалённое хранилище
dvc remote add -d
dvc push
Проверьте возможность переключения версий!

 

В README обновите:
где физически лежат данные/модели; 
git clone … && dvc pull && dvc repro
План экспериментов
Итого, репозиторий должен:

- хранить большие файлы вне Git, но под версионированием DVC; 

- позволять полностью восстановить любую версию датасета, модели и пайплайна одной командой dvc pull.

 

Задание 5


Интегрируем MLflow для трекинга экспериментов

 

В проекте должны быть реализованы следующие этапы:

Установите mlflow и добавьте его в requirements.txt/pyproject.toml.
В скрипте train.py:
локальное mlruns/ (по умолчанию) или удалённое (PostgreSQL + S3/MinIO). 
оберните обучение в контекст 
при использовании transformers, sklearn подключите mlflow.<lib>.autolog(). 
Настройте бекенд-хранилище
Обязательно убедитесь, что каждый запуск python train.py … создаёт отдельный run с:
параметрами; 
метриками; 
артефактами (model, plots, dvc.lock и др.).
В README обновите:
где смотреть результаты; 
как подключиться к удалённому серверу (если настроен).
Опционально(!!!): привяжите DVC + MLflow:
сохраняйте dvc.lock как артефакт; 
логируйте хеши датасета/модели
mlflow.set_tag("dvc_data_hash", <hash>) 
Итого, у вас

- каждая версия эксперимента фиксируется в MLflow; 

- все параметры, метрики и артефакты доступны через UI.

Задание 6

 

Упаковываем в Docker-образ 

Добавьте в проект

Dockerfile:
скопируйте requirements.txt и выполните pip install -r requirements.txt
скопируйте исходный код проекта + сохранённую модель (или подтяните её - dvc pull), объявите ENTRYPOINT ["python", "-m", "src.predict"]
Скрипт src/predict.py при запуске контейнера должен:
загрузить модель с диска 
принять аргументы командной строки --input_path и --output_path 
считать данные из input_path, выполнять predict, сохранять результаты в output_path 
Соберите образ docker build -t ml-app:v1
Запустите контейнер, пробросив файлы с данными
Убедитесь, что preds.csv содержит валидные предсказания 
При необходимости пропишите *.csv, *.pkl, *.pt в .dockerignore 
Обновите README: 
как построить образ и запустить контейнер 
что именно делает скрипт и какие форматы данных ожидает/возвращает   
Итого, должен быть воспроизводимый Docker-образ (ml-app:v1), запускающий офлайн-инференс одной командой.

 

Задание 7

 

Развёртываем модель как онлайн-сервис c TorchServe 
 

Подготовьте артефакты для TorchServe: 
сохраните модель в формат state_dict или torchscript (model.pt)
реализуйте пользовательский обработчик handler.py (пред- и постобработка) 
Создайте архив модели (torch-model-archiver). В результате получится model-store/mymodel.mar. 
Опишите Dockerfile на базе pytorch/torchserve:latest
Соберите образ: 
docker build -t mymodel-serve:v1 . 
Запустите контейнер в фоне, например: 
docker run -d -p 8080:8080 -p 8081:8081 mymodel-serve:v1 
Проверьте работу сервиса: 
curl -X POST http://localhost:8080/predictions/mymodel -T sample_input.json 
Обновите README:
как собрать/запустить контейнер TorchServe;
примеры REST-запросов и форматы данных;
параметры конфигурации сервиса.
Итого, получаем готовый Docker-контейнер, который автоматически поднимает TorchServe и регистрирует модель, и сервис, отвечающий на REST-запросы

Критерии оценки (максимум - 10 базовых баллов и 3 бонусных)

 

DVC-пайплайн и управление данными— до 2 баллов

Репозиторий инициализирован dvc init, крупные файлы вынесены из Git   
dvc содержит минимум три стадии, зависимости и выводы прописаны корректно 
Настроен удалённый remote, команда dvc push выполняется без ошибок, полный recovery git clone && dvc pull && dvc repro восстанавливает результат 
Трекинг экспериментов в MLflow— до 2 баллов

В train.py используется mlflow.start_run() или autolog, логируются параметры, метрики, артефакты (модель, графики) 
MLflow-хранилище (локальное или удалённое) конфигурировано, UI открывается и содержит запуски 
Хеши dvc или dvc.lock обспечивают связь данных и эксперимента 

Контейнер для офлайн-инференса (Docker) — до 2 баллов

Dockerfile, в котором копируются requirements.txt и устанавливаются зависимости, копируется код + модель (или выполняется dvc pull)  
 dockerignore исключает лишние данные/артефакты, образ собирается <1 ГБ 

Онлайн-сервис на TorchServe — до 2 баллов

Собран .mar-архив через torch-model-archiver, кастомный handler.py (при необходимости) лежит в репо 
Dockerfile наследуется от pytorch/torchserve, config.properties либо параметры cmd запускают модель 

Качество кода, тесты и автоматизация— до 2 баллов

Чистая структура проекта, понятные имена, фиксация random_seed 
Есть хотя бы базовые unit/integration-тесты на ключевые функции, тесты запускаются локально и в CI 
Настроен CI-workflow (GitHub Actions, GitLab CI и т.д.), который собирает образ(ы) и прогоняет тесты 
 